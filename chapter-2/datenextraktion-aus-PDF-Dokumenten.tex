\section{Datenextraktion aus PDF-Dokumenten}

\subsection{Das Portable Document Format (PDF)}

Das \emph{Portable Document Format (PDF)} ist eines der beliebtesten elektronischen Dokumentenformate.  
Das PDF-Format ist primär ein \emph{layoutbasiertes Format}. 
Es wurde entwickelt, um das Erscheinungsbild der Originaldokumente plattform- und anwendungsübergreifend zu bewahren. \cite{Lovegrove1995}
Das Format beschreibt Objekte auf einer niedrigen Strukturebene und legt die \emph{Positionen und Schriftarten der einzelnen Zeichen} fest, aus
denen der sichtbare Text zusammengesetzt ist. 
Zu den beschriebenen Objekten gehören:
\begin{itemize}
    \item Gruppen von Zeichen (Textobjekte)
    \item Linien, Kurven und Bilder
    \item Stilattribute wie Schriftart, Farbe, Strichführung, Füllung und geometrische Formen.
\end{itemize}
\cite{Bast2017}

Obwohl PDF die visuelle Darstellung eines Dokuments zuverlässig bewahrt, fehlt den meisten Dateien eine explizite logische Struktur auf höherer Ebene. 
Die folgenden semantischen Einheiten sind im Format \emph{nicht direkt enthalten} und nur durch die oben genannte niedrige Strukturebene zusammengesetzt:
\begin{itemize}
    \item logische Komponenten wie Wörter, Textzeilen, Absätze, Tabellen oder Abbildungen \cite{Chao2004}
    \item Informationen über die \emph{semantischen Rollen} des Textes (z.\,B.\ Haupttext, Fußnote oder Bildunterschrift), \cite{Bast2017}
    \item eine eindeutige Lese- und Wortreihenfolge, insbesondere bei mehrspaltigen Layouts oder eingebetteten Elementen. \cite{Bast2017}
\end{itemize}

Hinzuzufügen ist, dass PDF-Dokumente mit semantischen Informationen \emph{getaggt} werden können. In der Praxis sind diese zusätzlichen Informationen selten gegeben.
Die für diese Arbeit relevanten PEP-Ecopassport-PDFs sind alle nicht getaggt. \cite{Bast2017}

Das Fehlen dieser semantischen Informationen erschwert die Wiederverwendung, Bearbeitung oder Modifikation des Layouts und Inhalts erheblich. \cite{Corrêa2017}
Die automatische Extraktion dieser Metadaten und Textinhalte ist daher eine zentrale, aber fehleranfällige Aufgabe, da es keine 
allgemein verbindlichen Standards für die Strukturierung solcher Informationen in PDF-Dokumenten gibt. \cite{Lipinski2013}





\subsection{Herausforerungen bei der automatisierten Extraktion}

Die Rekonstruktion des Textflusses und der semantischen Einheiten aus den Positionen einzelner Zeichen ist komplex.

\subparagraph{1. Wortidentifikation}
Die korrekte Bestimmung von Wortgrenzen ist nicht trivial:
\begin{itemize}
    \item \emph{Abstände:} Die Abstände zwischen Zeichen können innerhalb einer Zeile variieren, sodass keine feste Regel existiert, 
    um Wortgrenzen ausschließlich anhand der Zeichenpositionen zu bestimmen. \cite{Bast2017}
    \item \emph{Silbentrennung:} In mehrspaltigen Layouts getrennte Wörter müssen korrekt wieder zusammengeführt werden. \cite{Bast2017}
    \item \emph{Ligaturen:} Zeichenkombinationen wie „fl“ oder „fi“ werden im PDF oft als einzelnes Zeichen gespeichert und 
    müssen beim Extrahieren in mehrere Zeichen aufgelöst werden. \cite{Lipinski2013}
    \item \emph{Diakritische Zeichen:} Buchstaben mit Diakritika (z.\,B.\ à, ã) können als zwei separate Zeichen gespeichert sein
    und müssen beim Parsing zu einem Zeichen zusammengeführt werden. \cite{Bast2017}
\end{itemize}

\subparagraph{2. Lesereihenfolge (Reading Order)}
Die korrekte Lesereihenfolge ist entscheidend für die Verständlichkeit des Textes und der weiterführenden Interpretation.  \cite{Bast2017}
In mehrspaltigen Layouts sind Textzeilen im PDF häufig in einer verschränkten Reihenfolge gespeichert. 
Ohne Korrekturmechanismen führt dies zu unleserlichem, inhaltlich falsch zusammengesetztem Text.\cite{Lovegrove1995}

\subparagraph{3. Absatzgrenzen (Paragraph Boundaries)}
Die Erkennung von Absatzanfängen und -enden ist besonders schwierig:
\begin{itemize}
    \item \emph{Unterbrechungen:} Text, der zu einem Absatz gehört, kann durch Formeln, Tabellen oder Abbildungen unterbrochen und später auf derselben Seite 
    fortgesetzt werden.
    \item \emph{Seitenumbrüche:} Absätze können am Seiten- oder Spaltenende abgeschnitten und auf der folgenden Seite fortgeführt werden, ohne dass dies
    im PDF strukturell kenntlich gemacht wird.
\end{itemize}
\cite{Bast2017}

\subparagraph{4. Technologische und Layout-Herausforderungen}
\begin{itemize}
    \item \emph{Überlagerungen (Overlays):} 
    In grafisch komplexen Dokumenten können Text- und Bildelemente überlappen, etwa wenn Beschriftungen in Abbildungen eingebettet sind. 
    Dies erschwert die korrekte Segmentierung. \cite{Chao2004}
    \item \emph{Segmentierungsfehler:} 
    Bei Tabellen, Karten oder Diagrammen kann Text aus unterschiedlichen logischen Einheiten fälschlicherweise in dieselbe Gruppe aggregiert werden. \cite{Chao2004}
    \item \emph{Type-3-Fonts:} 
    Manche Zeichen (insbesondere Ligaturen und Sonderzeichen) werden im PDF nicht als Textobjekte, sondern als Vektorgrafiken gespeichert. 
    Solche Elemente sind mit herkömmlicher Textextraktion nicht identifizierbar und erfordern erweiterte, teils OCR- oder ML-basierte Verfahren. \cite{Bast2017}
\end{itemize}


\subsection{Extraktionsansätze}
Da diese Probleme weit verbreitet und bekannt sind, ergeben sich mehrere Extraktionsansätze, die das Ziel PDF-Dateien in ein strukturierts Format zu bringen,
um sie anschließend weiter zu analysieren. 

\paragraph{Klassische Verfahren (regelbasierte Parser)}
Ein Beispiel für ein klassisches, regelbasiertes Extraktionstool ist die Open-Source-Bibliothek \emph{pdfplumber}. 
Sie ist vollständig in Python implementiert und baut auf der weit verbreiteten Bibliothek \emph{pdfminer.six} auf. 
Das Werkzeug wurde speziell für die Text- und Tabellenextraktion aus PDF-Dokumenten entwickelt und gilt als eine 
der benutzerfreundlichsten Lösungen im Python-Ökosystem. \cite{Adhikari2025}

\emph{pdfplumber} konvertiert beim Einlesen einer PDF-Datei deren Inhalt in ein analysierbares Python-Objekt, das sämtliche Seiteninformationen
wie Text, Linien, Rechtecke und Bilder enthält. Jede Seite wird dabei als Sammlung geometrischer Objekte behandelt, deren Koordinaten und Stilattribute 
(z.\,B.\ Schriftart, Farbe, Position) präzise erfasst sind. \cite{Yang2025} 
Diese Informationen werden über Objektlisten verfügbar gemacht. 
Für die Erkennung und Extraktion von Tabellen nutzt das Tool einfache visuelle Heuristiken: Standardmäßig werden horizontale und vertikale Linien
einer Seite als potenzielle Zellgrenzen interpretiert.  \cite{Yang2025}
Über Parameter wie \texttt{table\_settings} oder \texttt{snap\_tolerance} lässt sich die Erkennung an unterschiedliche Layouts anpassen. 
So können beispielsweise die Toleranzen für Linienabstände verändert werden, um verschobene Spalten oder leere Zellen zu korrigieren. \cite{Yang2025}

Der regelbasierte Ansatz von \emph{pdfplumber} führt bei klar strukturierten, editierbaren PDF-Dokumenten zu guten Ergebnissen. 
In Studien zur Leistungsbewertung von Extraktionstools erzielte es in Domänen wie juristischen oder technischen
Dokumenten hohe F1-Scores (beispielsweise 0{,}98 im Bereich \emph{Law}). \cite{Adhikari2025}

Die Grenzen des Werkzeugs zeigen sich jedoch vor allem bei komplexen oder unregelmäßig formatierten PDF-Dateien. 
Insbesondere wissenschaftliche Dokumente mit mathematischen Ausdrücken, eingebetteten Formeln oder verschachtelten Tabellen führen zu deutlichen Leistungseinbußen. 
In der Kategorie \emph{Scientific} sank der F1-Score auf 0{,}76, was vor allem auf unvollständige Tabellenerkennung und fehlerhafte Segmentierung zurückzuführen ist. 
Auch bei Patenten oder Dokumenten mit grafischen Strukturen (z.\,B.\ chemischen Formeln oder Bauzeichnungen) stößt der regelbasierte Ansatz an seine Grenzen. \cite{Adhikari2025}

Aus sicht der zuvor beschriebenen Herausforderungen adressiert \emph{pdfplumber} also Probleme auf der Ebene der Zeichen- und Worterkennung weitgehend.
Die Wiederherstellung der Lesereihenfolge erfolgt allerdings rein geometrisch, ohne semantisches Verständnis, wodurch Textpassagen aus mehrspaltigen Layouts7
häufig in falscher Reihenfolge extrahiert werden. 
Absatzgrenzen, semantische Rollen (z.\,B.\ Überschriften, Fließtext, Bildunterschriften) und komplexe Tabellenstrukturen erkennt \emph{pdfplumber} nicht zuverlässig.

Damit steht \emph{pdfplumber} exemplarisch für klassische Extraktionstools, die ohne maschinelles Lernen oder tiefere Dokumentenverständnis-Modelle
arbeiten und deshalb bei komplexen, visuell strukturierten Dokumenten wie den PEP-Ecopassports an ihre methodischen Grenzen stoßen.



\paragraph{Erweiterte Verfahren (z.B. Docling)}
Ein modernes, KI-gestütztes Gegenbeispiel zu klassischen, regelbasierten Extraktionstools ist das Open-Source-Toolkit \emph{Docling}. 
Es wurde mit dem Ziel entwickelt, PDF-Dokumente und andere Formate in eine maschinell verarbeitbare, reich strukturierte Repräsentation zu überführen. 
Im Gegensatz zu Werkzeugen wie \emph{pdfplumber}, die auf geometrischen Heuristiken basieren, kombiniert \emph{Docling} klassische Parsing-Verfahren mit 
tiefen neuronalen Modellen für Layout- und Strukturerkennung. \cite{Auer2024}
Das Toolkit ist vollständig in Python implementiert, modular aufgebaut und kann lokal ohne Cloud-Anbindung ausgeführt werden, 
was es insbesondere für den Einsatz in sensiblen Datenumgebungen geeignet macht. \cite{Auer2025}

Technisch basiert \emph{Docling} auf einer linearen Verarbeitungs-Pipeline, die mehrere spezialisierte Komponenten kombiniert. 
Nach dem initialen Parsen durch ein PDF-Backend (z.\,B.\ \emph{qpdf} oder \emph{pypdfium}) werden für jede Seite Bitmap-Abbilder erzeugt, 
auf denen KI-Modelle für Layout- und Strukturerkennung ausgeführt werden. \cite{Auer2024}
Das zugrunde liegende Layout-Analysemodell \emph{DocLayNet} identifiziert auf Basis eines trainierten Objektdetektors verschiedene 
Seitenelemente und deren Begrenzungsrahmen – etwa Absätze, Überschriften, Listen, Abbildungen oder Tabellen.  \cite{Auer2024}
Diese visuellen Einheiten werden mit den extrahierten Text-Tokens verknüpft und zu konsistenten Dokumentstrukturen zusammengeführt. 
Für erkannte Tabellenobjekte kommt anschließend das Vision-Transformer-Modell \emph{TableFormer} zum Einsatz, das die logische Zeilen- und Spaltenstruktur 
einer Tabelle rekonstruiert und die Zellen semantisch klassifiziert (z.\,B.\ Kopf- oder Körperzellen).  
Für gescannte oder bildbasierte Dokumente steht optional eine OCR-Komponente auf Basis von \emph{EasyOCR} zur Verfügung. \cite{Auer2025}

Das Herzstück von \emph{Docling} bildet das Datenmodell \emph{DoclingDocument}, eine vereinheitlichte interne Repräsentation, die sämtliche 
Inhalte eines Dokuments (Text, Tabellen, Bilder, Layoutinformationen, Hierarchieebenen und Metadaten) in strukturierter Form abbildet. 
Nach Abschluss aller Erkennungsschritte werden die Ergebnisse zu einem vollständigen \emph{DoclingDocument} zusammengeführt und können in 
verschiedenen Formaten exportiert werden, darunter JSON, Markdown und HTML. 
Im Post-Processing ergänzt ein sprachsensitives Modell weitere Merkmale wie die Korrektur der Lesereihenfolge, die automatische Spracherkennung und die 
Extraktion zentraler Metadaten (Titel, Autoren, Referenzen).
\cite{Auer2024}

Durch diese Architektur adressiert \emph{Docling} mehrere der in Abschnitt~2.3 beschriebenen Extraktionsprobleme, die klassische Tools nur unzureichend lösen können. 
Es rekonstruiert eine konsistente Lesereihenfolge auch bei mehrspaltigen Layouts, erkennt logische Dokumentstrukturen und kann Tabellen semantisch 
interpretieren, anstatt sie rein geometrisch zu segmentieren.  \cite{Auer2025}
Darüber hinaus bietet es eine robuste Metadaten- und Inhaltsklassifizierung, die zwischen Fließtext, Überschriften, Listen, 
Bildunterschriften und Formeln unterscheidet. 
Die erzeugten Ausgaben sind reich strukturiert und dienen als Grundlage für weiterführende Analysen oder Datenpipelines, etwa zur Wissensextraktion, 
semantischen Suche oder automatisierten Inhaltsklassifikation. \cite{Auer2024}

Im Vergleich zu klassischen, regelbasierten Parsern wie \emph{pdfplumber} kombiniert \emph{Docling} geometrische und visuelle Merkmale mit semantischem Verständnis. 
Dadurch ermöglicht es eine qualitativ hochwertige, KI-gestützte Dokumentenkonvertierung, die sowohl schnelle 
als auch stabile Ergebnisse liefert und für komplexe Dokumente wie PEP-Ecopassports einen erheblichen Qualitätsgewinn in der Extraktion bietet. \cite{Auer2024}

Tabelle~\ref{tab:extraktion_vergleich} fasst die Extraktionsfähigkeiten der beiden Ansätze zusammen. 

\begin{table}[h!]
\centering
\caption{Vergleich der Extraktionsfähigkeiten von \textit{pdfplumber} und \textit{Docling}}
\label{tab:extraktion_vergleich}
\begin{tabular}{p{5cm}p{4cm}p{4cm}}
\hline
\textbf{Extraktionsaspekt} & \textbf{pdfplumber (regelbasiert)} & \textbf{Docling (KI-gestützt)} \\
\hline
Zeichen- und Worterkennung & Gut: präzise Koordinaten- und Schriftanalyse, robust für editierbare PDFs & Gut: kombiniert geometrische und visuelle Merkmale, tolerant gegenüber Layoutabweichungen \\
Lesereihenfolge (Reading Order) & Schlecht: rein geometrisch, keine semantische Korrektur bei mehrspaltigen Layouts & Gut: Layout-Analysemodell erkennt Spalten, Absätze und Lesefluss kontextsensitiv \\
Absatz- und Textstruktur & Teilweise: rekonstruiert Absätze heuristisch anhand von Zeilenabständen & Gut: erkennt logische Abschnitte, Überschriften, Listen und Beschriftungen \\
Tabellenerkennung & Teilweise: funktioniert bei klaren Linienstrukturen; scheitert bei verschachtelten oder visuellen Tabellen & Gut: rekonstruiert Tabellenstruktur semantisch über das \emph{TableFormer}-Modell \\
Grafiken und eingebettete Objekte & Nicht gelöst: keine Analyse von Abbildungen oder Vektorobjekten & Teilweise: erkennt Abbildungen und Bildunterschriften, jedoch keine inhaltliche Bildanalyse \\
Metadatenextraktion & Nicht gelöst: keine Erkennung oder Harmonisierung von Titel, Autor, Referenzen & Gut: Post-Processing extrahiert zentrale Metadaten und Sprachinformationen \\
Mehrsprachige Dokumente & Eingeschränkt: kein Sprachmodell integriert & Teilweise: automatische Spracherkennung und Layout-Korrektur im Post-Processing \\
Nicht-textuelle Inhalte (z.\,B.\ OCR) & Nicht gelöst: unterstützt nur editierbare PDFs & Gut: optionale OCR-Erkennung für gescannte Dokumente über \emph{EasyOCR} \\
Komplexe Layouts (mehrspaltig, technische Doks) & Schwach: häufige Fehlsegmentierung und falsche Lesereihenfolge & Gut: robuste Layout-Analyse durch \emph{DocLayNet}-Modell \\
Semantische Rollen (z.\,B.\ Caption, Footnote) & Nicht gelöst & Gut: semantische Klassifizierung unterschiedlicher Dokumentelemente \\
\hline
\end{tabular}
\end{table}






