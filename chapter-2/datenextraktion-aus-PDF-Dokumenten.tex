\section{Datenextraktion aus PDF-Dokumenten}
Da die PEP\--Ecopassport\--Umweltdaten ausschließlich in PDF\--Dateien veröffentlicht werden, 
besteht der erste Schritt darin, die Informationen zu extrahieren, um sie 
für die quantitative Analyse in ein
einheitlich strukturiertes und maschinenlesbares Format zu bringen. 
Dieser Prozess ist nicht trivial und bildet die Grundlage für die weitere Verarbeitung,
Strukturierung und Analyse der Umweltindikatoren. 


\subsection{PDF-Struktur und Herausforderungen der Textextraktion}
\label{sec:pdf-challenges}

\cite{Lovegrove1995} beschreibt PDF als ein weit verbreitetes, primär layoutbasiertes
Dokumentenformat. Das Format enthält die Platzierung und Darstellung von Textobjekten
sowie grafischen Elementen wie Linien, Kurven und Bildern inklusive Stilattributen
etwa Schriftart, Farbe oder Strichführung. Dadurch bleibt das visuelle Erscheinungsbild
eines Dokuments zuverlässig über alle Geräte erhalten.

Nach \cite{Bast2017} und \cite{Chao2004} fehlt dem PDF-Format eine
explizite logische Struktur auf höherer Ebene. Semantische Einheiten wie Wörter,
Textzeilen, Absätze oder Tabellen sind nicht direkt enthalten. Auch die Rolle
eines Textblocks, etwa als Haupttext, Überschrift oder Fußnote, ist nicht
eindeutig hinterlegt. Ebenso ist die Lese- und Wortreihenfolge, insbesondere bei
mehrspaltigen Layouts oder eingebetteten Elementen, nicht definiert.

Wie \cite{Corrêa2017} zeigt, erschwert das fehlende semantische
Strukturgerüst in vielen PDFs die automatische Erkennung und
Wiederverwendung von Layout und Inhalt deutlich, da
die Rekonstruktion des Textflusses und der semantischen Einheiten 
ausschließlich auf den Positionen einzelner Zeichen basiert.

Die folgenden Herausforderungen der PDF-Extraktion sind bekannt und in der Literatur gut
dokumentiert (vgl.\ \cite{Bast2017,Lipinski2013,Chao2004,Lovegrove1995}).
Explizit ergeben sich dabei vier typische Problemklassen:

\begin{itemize}
  \item \textbf{Wort- und Zeichenrekonstruktion:}
  Wortgrenzen sind aufgrund variierender Zeichenabstände uneindeutig.
  Zusätzlich müssen Silbentrennungen, Ligaturen und diakritische Zeichen
  korrekt zusammengeführt werden.

  \item \textbf{Lesereihenfolge:}
  In mehrspaltigen oder komplexen Layouts ist die interne Reihenfolge der
  Textobjekte häufig nicht identisch mit dem menschlichen Lesefluss, was ohne
  Korrektur zu falsch zusammengesetztem Text führt.

  \item \textbf{Absatz- und Blockstruktur:}
  Absatzgrenzen sind nicht explizit kodiert. Textblöcke können durch Tabellen,
  Formeln oder Abbildungen unterbrochen sein und über Seiten oder Spalten hinweg
  fortgesetzt werden.

  \item \textbf{Layout- und Rendering-Artefakte:}
  Überlagerungen, Segmentierungsfehler bei Tabellen und Diagrammen sowie
  als Grafiken gerenderte Zeichen (z.\,B.\ Type-3-Fonts) erschweren die
  zuverlässige Extraktion.
\end{itemize}


\subsection{Extraktionsansätze}
Da diese Probleme weit verbreitet und bekannt sind, gibt es mehrere 
Extraktionsansätze, um PDF-Dateien in ein strukturierts Format zu bringen,
mit dem Ziel sie anschließend weiter zu analysieren. 


\paragraph{Klassische Verfahren (regelbasierte Parser)}
Regelbasierte PDF\--Parsing\--Methoden arbeiten mit
fest definierten Regeln und benötigen kein Training. 
Mehrere weit verbereutete Softwarebibliotheken implementieren regelbasierte Parser, 
darunter \emph{pdfplumber}, \emph{pypdfium2} und \emph{pypdf}.
Da die Arbeit von Selg auf \emph{pdfplumber} aufbaut, wird diese Bibliothek näher besprochen \cite{Selg2025}. 

Die Eigenschaften und Grenzen des Tools werden in \cite{Yang2025} und \cite{Adhikari2025} beschrieben:

Jede Seite wird als Sammlung
von Textfragmenten, Linien, Rechtecken und Bildern mit ihren Positionen in Python-Objekte gespeichert.
Für die Tabellenerkennung werden horizontale und vertikale
Linien als potenzielle Zellgrenzen interpretiert.

Bei klar strukturierten, 
editierbaren PDF-Dokumenten führt diese Methodik zu guten Ergebnissen. 
In Studien zur Leistungsbewertung von Extraktionstools erzielte es in Domänen 
wie juristischen oder technischen Dokumenten hohe F1-Scores 
(beispielsweise 0{,}98 im Bereich \emph{Law}).

Die Grenzen des Werkzeugs zeigen sich bei komplexen oder unregelmäßig
formatierten PDF-Dateien. Insbesondere wissenschaftliche Dokumente mit mathematischen Ausdrücken 
und verschachtelten Tabellen führen zu deutlichen Leistungseinbußen. 
In der Kategorie \emph{Scientific} sank der F1-Score auf 0{,}76, was vor allem auf unvollständige 
Tabellenerkennung und fehlerhafte Segmentierung zurückzuführen ist. 

Aus Sicht der zuvor beschriebenen Herausforderungen adressiert \emph{pdfplumber} 
also Probleme auf der Ebene der Zeichen- und Worterkennung weitgehend.
Die Wiederherstellung der Lesereihenfolge erfolgt allerdings rein geometrisch, 
ohne semantisches Verständnis, wodurch Textpassagen aus mehrspaltigen Layouts
häufig in falscher Reihenfolge extrahiert werden. 
Absatzgrenzen, semantische Rollen (z.\,B.\ Überschriften, Fließtext, Bildunterschriften) 
und komplexe Tabellenstrukturen werden nicht zuverlässig erkannt.

Damit steht \emph{pdfplumber} exemplarisch für klassische Extraktionstools, die ohne maschinelles 
Lernen und Modellen zum Dokumentenverständnis arbeiten und deshalb bei komplex strukturierten Dokumenten 
wie den PEP-Ecopassports an ihre methodischen Grenzen stoßen.



\paragraph{Erweiterte Verfahren (z.B. Docling)}
Neben klas\-sischen re\-gel\-ba\-sier\-ten Parsern etablierten sich  
moderne, KI-gestützte Dokumentenanalyse-Frameworks. 
Dazu gehören komplexe Layout-Modelle wie \emph{LayoutParser} \cite{layout-parser}, 
\emph{GROBID} \cite{grobid}, sowie \emph{Docling}. 
Sie kombinieren visuelle Merkmale, Textinformationen und 
semantische Modelle, um Dokumente mit anspruchsvoller Struktur automatisch 
zu analysieren und in maschinenlesbare Formate zu überführen.

Für diese Arbeit wird \emph{Docling} näher betrachtet, da es ein lokal ausführbares 
Open-Source-Toolkit ist, das eine vollständige 
End-to-End-Pipeline für Layout-Analyse, Strukturerkennung und Tabellensegmentierung bereitstellt.

Die Funktion und Hintergründe von \emph{Docling} werden in der Dokumentation (vgl. \cite{Auer2024, Auer2025}) 
ausführlich beschrieben und im Folgenden zusammengefasst:
Im Gegensatz zu regelbasierten Werkzeugen wie \emph{pdfplumber}, 
kombiniert \emph{Docling} klassische Parsing-Verfahren mit 
tiefen neuronalen Modellen für Layout- und Strukturerkennung. 

\emph{Docling} verarbeitet Dokumente in einer linearen Pipeline. 
Das Layoutmodell \emph{DocLayNet} ist Teil dieser Pipeline und detektiert Seitenelemente wie Absätze,
Überschriften, Listen, Abbildungen und Tabellen, die anschließend mit den
extrahierten Text-Tokens zu einer konsistenten Dokumentstruktur
zusammengeführt werden. Tabellen werden mit
\emph{TableFormer} in ihrer Zeilen- und Spaltenlogik rekonstruiert und die
Zellen semantisch klassifiziert.

Die Ergebnisse
werden in einem \emph{DoclingDocument} gebündelt, das Inhalte, Layout und
Metadaten einheitlich repräsentiert und in Formate wie JSON, Markdown
oder HTML exportiert. Ein Post-Processing verbessert zudem die
Lesereihenfolge, erkennt die Sprache und extrahiert zentrale Metadaten.

Durch diese Architektur können zentrale Herausforderungen
adressiert werden (vgl.\ \ref{sec:pdf-challenges}). \emph{Docling}
rekonstruiert den Lesefluss auch bei mehrspaltigen Layouts, erkennt logische
Dokumentelemente und interpretiert Tabellen strukturell statt rein geometrisch.
Zusätzlich klassifiziert es Inhalte nach Rollen wie Fließtext, Überschrift oder
Bildunterschrift, wodurch die Ausgaben als strukturierte Basis für
weiterführende Analysen nutzbar sind.


\subsection{Zielformat JSON}

Die aus den PEP-PDFs extrahierten Inhalte liegen zunächst als Text oder
Markdown vor. Diese Formate sind zwar lesbar, liefern aber kein stabiles Schema,
um Umweltindikatoren, Materialien und Metadaten über viele Dokumente hinweg
eindeutig und konsistent zuzuordnen. Für die quantitative, automatisierte Auswertung und Analyse 
ist daher ein fest definiertes, maschinenlesbares Zielformat erforderlich.

Als Zielformat wird \emph{JavaScript Object Notation (JSON)} verwendet, das sich
als Standard für strukturierten Datenaustausch etabliert hat \cite{Pezoa2016}.
JSON bildet Daten über einfache Typen sowie hierarchische Strukturen wie
Objekte und Arrays ab und eignet sich damit für verschachtelte Informationen
\cite{Pezoa2016}. In dieser Arbeit dient JSON als einheitliche Repräsentation
der extrahierten PEP-Daten und ermöglicht eine reproduzierbare Weiterverarbeitung
in Analyseumgebungen wie Python oder R \cite{Pezoa2016}.



\subsection{Informationsextraktion von Markdown nach JSON}

In \cite{Grishman2015} wird Informationsextraktion (IE) als Aufgabe beschrieben,
aus unstrukturiertem Text gezielt relevante Informationen zu gewinnen und sie in
eine strukturierte Repräsentation wie ein JSON-Schema zu überführen. Für
diese Aufgabe lassen sich zwei Ansätze unterscheiden. Erstens klassische, regelbasierte Pipeline-Systeme. 
Zweitens Verfahren auf Basis großer
Sprachmodelle (LLMs).

\cite{Grishman2015} beschreibt die Grund\-sätze und 
He\-raus\-for\-derun\-gen tra\-ditio\-nel\-ler IE-Systeme.
Traditionelle IE-Systeme sind meist
mehrstufig aufgebaut, zum Beispiel mit Entitätserkennung, syntaktischer
Analyse, Koreferenzauflösung und Relations\-extraktion. Ein Vorteil dieser
Pipeline-Ansätze ist die hohe Nachvollziehbarkeit und die weitgehend
deterministische Verarbeitung. Bei heterogenen Textformaten können allerdings Fehler aus frühen Stufen leicht
in spätere Schritte übertragen werden und die Entwicklung robuster Regeln
oder Feature-Sets ist mit erheblichen Aufwand verbunden.

In \cite{Nadeem2024} werden große Sprachmodelle (LLMs) als neuere Möglichkeit
beschrieben, Informationsextraktion semantisch zu lösen. 
LLMs können Textpassagen kontextsensitiv interpretieren und
strukturierte Ausgaben direkt erzeugen. Wie
\cite{Filho2025} zeigt, lassen sich dabei Entitäten, Relationen und numerische
Werte häufig ohne manuelles Regelwerk oder domänenspezifisch annotiertes
Trainingskorpus zuordnen. Zudem können LLMs zuvor durch Tools wie
\emph{Docling} erzeugte Markdown- oder Textsegmente semantisch auswerten, sodass
eine strikt sequentielle Abarbeitung einzelner Pipeline-Stufen teilweise
entfällt \cite{Nadeem2024}. Ein zentraler Nachteil ist die fehlende
Deterministik und das Risiko von Halluzinationen, also falsch generierten
Werten, das sich durch präzises Prompt-Design und Validierungsschritte zwar
reduzieren, aber nicht vollständig ausschließen lässt \cite{Filho2025}.


Zusammenfassend sind regelbasierte Verfahren gut nachvollziehbar, stoßen bei den
heterogenen und layoutbedingt uneinheitlichen PEP-Texten jedoch schnell an
Grenzen. LLM-basierte Methoden sind flexibler und können Inhalte direkt in das
vorgegebene JSON-Schema überführen, erfordern dafür aber Validierungsschritte.
Da diese Arbeit auf eine quantitative Analyse der PEP-Ecopassport PDFs abzielt, wird trotzdem
ein LLM für die Extraktion und Überführung in das harmonisierte JSON-Zielformat eingesetzt.