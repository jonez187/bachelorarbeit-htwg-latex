\section{Statistische Grundlagen}
\label{sec:stat_grundlagen}

Die in dieser Arbeit verwendeten statistischen Verfahren bilden 
die methodische Grundlage zur Analyse und Modellierung der aus PEPs extrahierten Daten. 
Dazu werden zunächst \emph{deskriptive und explorative} Verfahren eingesetzt, um Strukturen, 
Streuungen und Ausreißer in den Daten sichtbar zu machen. 
Darauf aufbauend wird die \emph{lineare Regression} als einfaches, interpretierbares Modell genutzt, 
um heuristische Beziehungen zwischen Einflussgrößen und den resultierenden Umweltindikatoren zu identifizieren. 
Diese Kombination ermöglicht eine robuste, nachvollziehbare und datengetriebene Einschätzung ökologischer Wirkzusammenhänge
im Datensatz.

\FloatBarrier
\subsection{Deskriptive Statistik}
Die deskriptive Statistik bildet die Grundlage der quantitativen Datenanalyse. 
Sie dient der Zusammenfassung und Beschreibung von Datensätzen, 
um zentrale Merkmale einer Verteilung zu charakterisieren und potenzielle 
Muster oder Auffälligkeiten zu erkennen \cite{Fisher2009}. Der Schwerpunkt liegt nicht auf 
Hypothesentests, sondern auf dem Verständnis der vorhandenen Daten \cite{Dimić2019}. 

Die deskriptive Statistik umfasst numerische Verfahren zur Beschreibung
der \emph{Lage-} und \emph{Streuungskennzahlen} von Daten \cite{Fisher2009}. 
Ziel ist die Abbildung großer Datenmengen auf wenige aussagekräftige Kennzahlen. 
Zu den typischen Lagemaßen gehören \emph{Mittelwert} und \emph{Median}.
Der Mittelwert beschreibt die durchschnittliche Ausprägung, während der Median die geordnete Verteilung 
in zwei gleich große Hälften teilt. 
Der Median gilt als \emph{robustes Lagemaß}, da er, im Gegensatz zum Mittelwert, wenig durch Ausreißer beeinflusst wird.
Für die Streuung werden Standardabweichung, Spannweite und insbesondere der \emph{Interquartilsabstand (IQR)} verwendet.
Der IQR beschreibt die mittleren 50~\% der Daten und ist ein robustes Maß, das gegenüber Extremwerten stabil bleibt.
Für ordinale Merkmale ist der Median das geeignete Lagemaß. Der IQR, ergänzt um Minimum und Maximum, quantifiziert 
die Streuung \cite{Fisher2009}. 


Ein zentrales Merkmal numerischer Daten ist die Form ihrer Verteilung. 
In symmetrischen Verteilungen fallen Mittelwert und Median zusammen.
Bei \emph{rechtsschiefen} Verteilungen liegen einzelne hohe Werte weit über dem zentralen Bereich, 
sodass der Mittelwert größer als der Median ist. 
Bei \emph{linksschiefen} Verteilungen gilt das umgekehrte Muster \cite{Kaur2018}. 
Schiefe beeinflusst die Interpretation von Lage- und Streumaßen und motiviert den 
Einsatz robuster Kennwerte wie Median und IQR \cite{Marshall2010}.


\subsection{Explorative Datenanalyse und Visualisierungen}
Die explorative Datenanalyse ergänzt die deskriptive Statistik durch strukturentdeckende Verfahren. 
Sie dient der visuellen Erkundung und Bewertung von Mustern, Ausreißern oder Zusammenhängen zwischen Variablen, 
ohne dass zuvor Hypothesen formuliert werden müssen. 
Zentrale Visualisierungen sind Histogramme, Boxplots und QQ-Plots \cite{Kaur2018}. 

\emph{Histogramme} stellen Häufigkeitsverteilungen kontinuierlicher Merkmale über Klassen dar. 
Sie erlauben Rückschlüsse auf Symmetrie, Schiefe und Mehrgipfligkeit und dienen zur
Prüfung von Verteilungsannahmen \cite{Marshall2010}. 

\emph{Boxplots} visualisieren Median ($Q_2$), Quartile ($Q_1$, $Q_3$) und potenzielle Ausreißer.
Quartile teilen die Verteilung in vier gleich große Teile.
Die sogenannten \emph{Whisker} markieren üblicherweise den Bereich bis zum 1{,}5-fachen Interquartilsabstand. 
Werte außerhalb gelten als potenzielle Ausreißer \cite{Marshall2010}. 
Diese Darstellungsform ermöglicht die Beurteilung von Streuung, Schiefe und Extremwerten
und eignet sich für den Vergleich mehrerer Merkmale \cite{Kaur2018}. 

\emph{QQ-Plots (Quantile-Quantile-Plots)} untersuchen grafisch, wie gut ein Datensatz einer bestimmten 
theoretischen Verteilung folgt \cite{Seltman2018}. 
In dieser Arbeit werden sie eingesetzt, um zu prüfen, 
ob die Residuen eines Modells normalverteilt sind. 

\FloatBarrier
\subsection{Mathematische Transformationen}
Transformationen dienen nach \cite{Marshall2010} und \cite{Fisher2009} dazu, 
Schiefe zu reduzieren, Varianz zu stabilisieren und parametrische Tests zu ermöglichen, 
damit die Modellierung auf der Transformationsskala sinnvoller wird. 
Als eine gängige Transformation, um Rechtsschiefe zu korrigieren, wird die 
logarithmische Transformation (Log-Transformation) erwähnt.
Da der Logarithmus extrem hohe Werte stärker staucht als kleine Werte, 
kann die Transformation nach \cite{Marshall2010} den 
Einfluss von Ausreißern auf das Gesamtergebnis reduzieren.
Für sehr kleine Werte von $x$ nahe $0$ gilt die Näherung $\log(1+x) \approx x$.
Kleine Werte werden demnach kaum verändert. 

Neben der festen Log-Transformation kann nach \cite{Atkinson2021} auch die 
Box-Cox-Transformation verwendet werden, die eine Familie von Potenztransformationen 
mit dem Parameter $\lambda$ darstellt.
Für positive Daten ist sie definiert als
\[
y^{(\lambda)} =
\begin{cases}
\dfrac{y^\lambda - 1}{\lambda}, & \lambda \neq 0, \\
\log(y), & \lambda = 0.
\end{cases}
\]
Damit ist die Log-Transformation als Spezialfall für $\lambda = 0 $ enthalten.
Ansonsten gilt $\lambda = 1$ entspricht keiner Transformation und 
$\lambda = 1/2$ entspricht der Quadratwurzeltransformation.
Im Unterschied zu $\log(1+x)$ ist die Form der Transformation hier nicht fest vorgegeben, 
sondern wird über $\lambda$ angepasst.
Ziel ist es, eine Skala zu finden, auf der sich ein lineares Modell einfacher und mit 
besser erfüllten Verteilungsannahmen der Fehler beschreiben lässt \cite{Atkinson2021}.
In Softwarebibliotheken wird der Parameter $\lambda$ datengetrieben bestimmt.
In Python übernimmt dies beispielsweise die Bibliothek \texttt{scipy}, indem $\lambda$ 
intern per Maximum-Likelihood geschätzt wird \cite{scipy-boxcox}.

Die Wirkung der Transformationen wird in Abbildung \ref{fig:log_transform_hist} exemplarisch gezeigt.
Links ist die rechtsschiefe Verteilung auf Originalskala dargestellt, in der Mitte dieselben Werte 
nach der Transformation $\log(1+x)$ und rechts nach einer Box-Cox-Transformation.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{images/transform_demo_log1p_vs_boxcox_kde.png}
  \caption{Beispiel einer rechtsschiefen Verteilung vor und nach Transformationen. 
  Die überlagerte Dichtekurve verdeutlicht die Form der Verteilung. [Eigene Darstellung]}
  \label{fig:log_transform_hist}
\end{figure}

In dieser Arbeit wird vor allem die Transformation $\log(1+x)$ verwendet
(\texttt{log1p}), um Indikatorwerte und Residuen auf einer besser interpretierbaren
Skala zu analysieren.


\FloatBarrier
\subsection{Hauptkomponentenanalyse (PCA)}
Die Haupt\-kom\-ponenten\-analy\-se (Prin\-cipal Component Analysis, PCA) reduziert die
Komplexität multi\-variater Daten, indem neue Variablen, die \emph{Hauptkomponenten},
als Linearkombinationen der ursprünglichen Variablen konstruiert werden.
In \cite{Maćkiewicz1993} wird dieses Prinzip so beschrieben, dass die erste
Komponente die größtmögliche Varianz der Daten erfasst, während weitere
Komponenten orthogonal dazu definiert werden und jeweils möglichst viel der
verbleibenden Varianz erklären.

Eine anschauliche Interpretation liefert die geometrische Sichtweise.
In \cite{Abdi2010} wird erläutert, dass die Werte der Hauptkomponenten für jede
Beobachtung als Projektionen der Datenpunkte auf neue Achsen verstanden werden
können. Diese Hauptkomponenten lassen sich als gedrehte Achsen im Merkmalsraum
interpretieren. Genau das illustriert Abbildung~\ref{fig:pca_basiswechsel}.
Die Standardachsen $(x_1,x_2)$ beschreiben die Daten zunächst in der ursprünglichen
Basis. Die PCA wählt stattdessen eine neue orthogonale Basis $(v_1,v_2)$, wobei
$v_1$ entlang der Richtung maximaler Streuung der Punktwolke liegt. Die zweite
Achse $v_2$ steht senkrecht dazu und beschreibt die Reststreuung.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.85\textwidth]{images/pca_basiswechsel.png}
  \caption{PCA als Basiswechsel: Standardachsen $(x_1,x_2)$ und Hauptachsen
  $(v_1,v_2)$ der Punktwolke. [Eigene Darstellung]}
  \label{fig:pca_basiswechsel}
\end{figure}

Für viele Anwendungen genügt es, nur die ersten wenigen Komponenten
weiterzuverwenden, weil sie den dominanten Teil der Varianz tragen. Dadurch
vereinfachen sich Visualisierung und Modellierung, ohne dass die Datenstruktur
vollständig verloren geht. In \cite{Abdi2010} werden diese Ziele als
Informationsverdichtung und Strukturanalyse der Variablen
beschrieben.

In dieser Arbeit wird PCA als Baustein für die Principal Component Regression
genutzt. Dabei werden korrelierte Regressoren, hier insbesondere
Materialanteile, durch unkorrelierte Hauptkomponenten ersetzt. In
\cite{Jolliffe1982} wird dieses Vorgehen damit begründet, dass die Regression
durch orthogonale Prädiktoren stabiler werden kann. Dadurch lässt sich
Multikollinearität reduzieren, während das Modell weiterhin auf denselben
Eingangsinformationen basiert.


\FloatBarrier
\subsection{Lineare Regression}

Die lineare Regression dient in dieser Arbeit als methodische Grundlage zur 
Modellierung der Umweltwirkungen von Produkten auf Basis quantitativer Einflussgrößen. 
Ziel ist es, Zusammenhänge zwischen erklärenden Variablen wie \emph{Produktgewicht}, 
\emph{Materialzusammensetzung}, \emph{Stromverbrauch} und \emph{verwendetem Energiemix}
und den resultierenden \emph{Umweltindikatoren} zu erkennen und zur Abschätzung unbekannter 
Werte nutzbar zu machen.

\subsubsection{Regressionsmodell und Grundannahmen}

Das Regressionsmodell beschreibt den linearen Zusammenhang zwischen einer abhängigen Variable 
\( y \) (z.\,B.\ einem Umweltindikator)
und mehreren unabhängigen Variablen 
\( x_1, x_2, \dots, x_k \) (z.\,B.\ Gewicht, Stromverbrauch, Materialanteile):
\[
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_k x_k + \varepsilon.
\]
Dabei ist \( \beta_0 \) der Achsenabschnitt, \( \beta_i \) die Regressionskoeffizienten der 
jeweiligen Einflussgrößen und \( \varepsilon \) ein zufälliger Fehlerterm, der 
unerklärte Varianzanteile abbildet. Die Koeffizienten \(\beta_i\) quantifizieren die 
Richtung und Stärke des Einflusses einzelner Variablen auf den Zielindikator \cite{Montgomery2022}. 

Für die lineare Regression gelten folgende Grundannahmen:
\begin{itemize}
    \item \textbf{Linearität:} Die Beziehung zwischen der abhängigen und den unabhängigen Variablen ist näherungsweise linear.
    \item \textbf{Erwartungswert der Fehler:} Die Fehlerterme haben einen Erwartungswert von null \(E(\varepsilon)=0\) und sind normalverteilt.
    \item \textbf{Homoskedastizität:} Die Varianz der Fehler ist konstant und unabhängig von den erklärenden Variablen.
    \item \textbf{Unabhängigkeit:} Die Fehlerterme sind voneinander unkorreliert.
    \cite{XiaogangSu2012}
\end{itemize}
 
Diese Annahmen sichern die Unverzerrtheit und Effizienz der Parameterschätzungen. 
Für explorative Anwendungen, wie sie in dieser Arbeit verfolgt werden, steht jedoch die 
Struk\-tur\-ent\-deckung im Vor\-der\-grund.
Moderate Abweichungen von den Idealannahmen sind daher akzeptabel, sofern sie dokumentiert werden.

\subsubsection{Schätzung der Regressionskoeffizienten}
Die Schätzung der Regressionskoeffizienten erfolgt nach der Methode der kleinsten Quadrate (\emph{Ordinary Least Squares}, OLS). 
Dabei werden die Parameter so bestimmt, dass die Summe der quadrierten Abweichungen zwischen 
beobachteten und modellierten Werten minimal wird:
\[
S(\boldsymbol{\beta}) = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2.
\]
\cite{XiaogangSu2012}

Dabei ist $n$ die Anzahl der Beobachtungen, $y_i$ der beobachtete Zielwert und
$\hat{y}_i$ die Modellvorhersage für Beobachtung $i$.
Die Koeffizienten \(\beta_i\) geben an, wie stark sich der Zielindikator \(y\) im Mittel verändert, 
wenn sich die Einflussgröße \(x_i\) um eine Einheit ändert, während alle anderen Variablen konstant bleiben.

In praktischen Anwendungen können die OLS-Schätzer trotz unverzerrter Erwartung
eine hohe Varianz aufweisen, insbesondere bei Multikollinearität.
In diesen Fällen können \emph{Regularisierungsverfahren} die Vorhersagegüte verbessern,
indem sie große Koeffizienten gezielt bestrafen und dadurch stabilere Modelle erzeugen \cite{XiaogangSu2012}.

Die \emph{Ridge-Regression} (L2-Regularisierung) erweitert das OLS-Kriterium um einen
quadratischen Strafterm:
\[
\min_{\boldsymbol{\beta}}\ \sum_{i=1}^{n}(y_i - \hat{y}_i)^2\ +\ \lambda \sum_{j=1}^{k}\beta_j^2 .
\]
Der Regularisierungsparameter $\lambda \ge 0$ steuert die Stärke der Schrumpfung.
Mit wachsendem $\lambda$ werden die Koeffizienten stärker in Richtung null gezogen,
bleiben jedoch typischerweise ungleich null. Dies reduziert die Varianz der Schätzung,
akzeptiert dafür eine geringe Verzerrung und kann den Testfehler senken \cite{Montgomery2022}.

Die \emph{Lasso-Regression} (L1-Regularisierung) verwendet einen absoluten Strafterm:
\[
\min_{\boldsymbol{\beta}}\ \sum_{i=1}^{n}(y_i - \hat{y}_i)^2\ +\ \lambda \sum_{j=1}^{k}|\beta_j| .
\]
Durch die L1-Struktur können Koeffizienten exakt null werden.
Lasso ermöglicht damit zusätzlich eine implizite Variablenselektion und führt häufig zu
sparsameren, leichter interpretierbaren Modellen \cite{XiaogangSu2012}.

Der Parameter $\lambda$ wird in der Regel in beiden Re\-gu\-la\-ri\-sierungs\-me\-tho\-den da\-ten\-getrieben
bestimmt, etwa mittels
Cross Validation auf Trainingsdaten. Dadurch wird ein Kompromiss zwischen
Unteranpassung bei zu großer Regularisierung und Überanpassung bei $\lambda = 0$ gewählt \cite{XiaogangSu2012}.


\subsubsection{Residuen und Gütemaße}
Ein \emph{Residuum} \(e_i\) ist die Abweichung zwischen einem beobachteten Wert \(y_i\) und dem vom Modell vorhergesagten
(angepassten) Wert \(\hat{y}_i\) und wird definiert als
\[
e_i = y_i - \hat{y}_i .
\]
Residuen können als beobachtbare Werte der Modellfehler verstanden.

Das \emph{Bestimmtheitsmaß} \(R^2\) beschreibt den Anteil der Varianz des Zielindikators, der durch die 
erklärenden Variablen erklärt wird, und dient als zentrales Maß der Modellgüte. 
Der \emph{Root Mean Square Error} (RMSE) misst die durchschnittliche Abweichung zwischen 
beobachteten und vorhergesagten Werten. Er hat die gleiche Einheit wie die Zielvariable und 
ist dadurch leicht interpretierbar und ebenfalls ein Indikator für die Modellgüte \cite{Montgomery2022}. 

In dieser Arbeit wird die multiple lineare Regression verwendet, 
um Heuristiken zur Abschätzung der Umweltwirkungen von Elektro- und Elektronikprodukten zu entwickeln.
Das Modell dient der quantitativen Erfassung
von Zusammenhängen zwischen Produktmerkmalen und Umweltindikatoren und daraus schließend der möglichst präzisen Prognose 
der Umweltindikatoren anhand der Input-Variablen.
Damit bildet die lineare Regression eine nachvollziehbare, statistisch fundierte Basis für die 
Entwicklung eines vereinfachten Bewertungsmodells innerhalb der PEP-Datenanalyse.
