\section{Statistische Grundlagen}
\label{sec:stat_grundlagen}

Die in dieser Arbeit verwendeten statistischen Verfahren bilden 
die methodische Grundlage zur Analyse und Modellierung der aus PEP~Ecopassports extrahierten Daten. 
Dazu werden zunächst \emph{deskriptive und explorative} Verfahren eingesetzt, um Strukturen, 
Streuungen und Ausreißer in den Daten sichtbar zu machen. 
Die \emph{Hauptkomponentenanalyse} wird dafür verwendet die wichtigsten Merkmale zu identifizieren.
Darauf aufbauend wird die \emph{lineare Regression} als einfaches, interpretierbares Modell genutzt, 
um heuristische Beziehungen zwischen Einflussgrößen und den resultierenden Umweltindikatoren zu identifizieren. 
Diese Kombination ermöglicht eine robuste, nachvollziehbare und datengetriebene Einschätzung ökologischer Wirkzusammenhänge
im Datensatz.

\subsection{Deskriptive und explorative Statistik}
Die deskriptive und explorative Statistik bilden die Grundlage der quantitativen Datenanalyse. 
Beide dienen der Zusammenfassung, Beschreibung und Visualisierung von Datensätzen, 
um zentrale Merkmale einer Verteilung zu charakterisieren und potenzielle Muster oder Auffälligkeiten zu erkennen. \cite{Fisher2009}
Der Schwerpunkt liegt nicht auf Hypothesentests, sondern auf dem Verständnis der vorhandenen Daten. \cite{Dimić2019}

\paragraph{Deskriptive Statistik}
Die deskriptive Statistik umfasst numerische und grafische Verfahren zur Beschreibung
der \emph{Lage} und der \emph{Streuungskennzahlen} von Daten. \cite{Fisher2009}
Ziel ist die Abbildung großer Datenmengen auf wenige aussagekräftige Kennzahlen. 
Zu den typischen Lagemaßen gehören \emph{Mittelwert}, \emph{Median} und \emph{Modalwert}.
Der Mittelwert beschreibt die durchschnittliche Ausprägung, während der Median die geordnete Verteilung 
in zwei gleich große Hälften teilt. 
Der Median gilt als \emph{robustes Lagemaß}, da er, im Gegensatz zum Mittelwert, wenig durch Ausreißer beeinflusst wird.
Der Modalwert ist der Wert, der in der Stichprobe am häufigsten vorkommt. \cite{Dimić2019}
Für die Streuung werden Standardabweichung, Spannweite und insbesondere der \emph{Interquartilsabstand (IQR)} verwendet.
Der IQR beschreibt die mittleren 50~\% der Daten und ist ein robustes Maß, das gegenüber Extremwerten stabil bleibt.
Für ordinale Merkmale ist der Median das geeignete Lagemaß. Der IQR, ergänzt um Minimum und Maximum, quantifiziert 
die Streuung. \cite{Fisher2009}

\paragraph{Verteilungsformen und Schiefe}
Ein zentrales Merkmal numerischer Daten ist die Form ihrer Verteilung. 
In symmetrischen Verteilungen fallen Mittelwert, Median und Modalwert zusammen.
Bei \emph{rechtsschiefen} Verteilungen liegen einzelne hohe Werte weit über dem zentralen Bereich, sodass der Mittelwert größer als der Median ist; 
bei \emph{linksschiefen} Verteilungen gilt das umgekehrte Muster. \cite{Kaur2018}
Schiefe beeinflusst die Interpretation von Lage- und Streumaßen und motiviert den Einsatz robuster Kennwerte wie Median und IQR.
In der explorativen Praxis werden zudem log-transformierte Werte betrachtet, 
um stark asymmetrische Verteilungen zu symmetrisieren und visuell leichter interpretierbar zu machen.\cite{Marshall2010}

\iffalse
\paragraph{Log-Transformation und methodische Alternativen}
Wenn eine Verteilung deutlich von einer Normalverteilung abweicht, bestehen drei grundlegende Optionen:
(i) eine regelbasierte \emph{Ausreißerprüfung} mit dokumentierter Entfernung, \cite{Fisher2009}
(ii) eine \emph{Log-Transformation} zur Annäherung an Symmetrie und zur besseren Vergleichbarkeit in Visualisierungen \cite{Marshall2010},
oder (iii) die Anwendung \emph{nicht-parametrischer} Verfahren, die keine Normalverteilung voraussetzen \cite{Fisher2009}.
Die Entscheidung erfolgt im Rahmen der explorativen Analyse.
\fi

\subsection{Explorative Datenanalyse und Visualisierungen}
Die explorative Datenanalyse ergänzt die deskriptive Statistik durch strukturentdeckende Verfahren. 
Sie dient der visuellen Erkundung und Bewertung von Mustern, Ausreißern oder Zusammenhängen zwischen Variablen, 
ohne dass zuvor Hypothesen formuliert werden müssen. 
Zentrale Visualisierungen sind Histogramme, Boxplots und QQ-Plots. \cite{Kaur2018}

\emph{Histogramme} stellen Häufigkeitsverteilungen kontinuierlicher Merkmale über Klassen dar. 
Sie erlauben Rückschlüsse auf Symmetrie, Schiefe und Mehrgipfligkeit und dienen zur
Prüfung von Verteilungsannahmen. \cite{Marshall2010}

\emph{Boxplots} visualisieren Median ($Q_2$), Quartile ($Q_1$, $Q_3$) und potenzielle Ausreißer.
Die sogenannten \emph{Whisker} markieren üblicherweise den Bereich bis zum 1{,}5-fachen Interquartilsabstand. 
Werte außerhalb gelten als potenzielle Ausreißer. \cite{Marshall2010} 
Diese Darstellungsform ermöglicht die Beurteilung von Streuung, Schiefe und Extremwerten
und eignet sich für den Vergleich mehrerer Merkmale. \cite{Kaur2018}

\emph{QQ-Plots (Quantile-Quantile Plots)} untersuchen grafisch, wie gut ein Datensatz einer bestimmten 
theoretischen Verteilung folgt \cite{Seltman2018}. 
In dieser Arbeit werden sie eingesetzt, um zu prüfen, 
ob ein Datensatz normalverteilt ist. 

\subsection{Mathematische Transformationen}
Um reale Daten einer Normalverteilung anzunähern und somit die Nutzung parametrischer Tests zu ermöglichen, 
werden nach \cite{Marshall2010} und \cite{Fisher2009} mathematische Transformationen angewendet.
Die logarithmische Transformation (Log-Transformation) wird als eine der gängigsten Methoden, um Rechtsschiefe zu korrigieren, 
erwähnt.
Da der Logarithmus extrem hohe Werte stärker staucht als kleine Werte, kann die Transformation nach \cite{Marshall2010} den 
Einfluss von Ausreißern auf das Gesamtergebnis reduzieren.
Für sehr kleine Werte von $x$ nahe $0$ gilt die Näherung $\log(1+x) \approx x$.
Kleine Werte werden demnach kaum verändert. 

Neben der festen Log-Transformation kann nach \cite{Atkinson2021} auch die 
Box-Cox-Transformation verwendet werden, die eine Familie von Potenztransformationen 
mit dem Parameter $\lambda$ darstellt.
Für positive Daten ist sie definiert als
\[
y^{(\lambda)} =
\begin{cases}
\dfrac{y^\lambda - 1}{\lambda}, & \lambda \neq 0, \\
\log(y), & \lambda = 0.
\end{cases}
\]
Damit ist die Log-Transformation als Spezialfall für $\lambda = 0 $ enthalten.
Ansonsten gilt $\lambda = 1$ entspricht keiner Transformation und 
$\lambda = 1/2$ entspricht der Quadratwurzeltransformation.
Im Unterschied zu $\log(1+x)$ ist die Form der Transformation hier nicht fest vorgegeben, 
sondern wird über $\lambda$ angepasst.
Ziel ist es, eine Skala zu finden, auf der sich ein lineares Modell einfacher und mit 
besser erfüllten Verteilungsannahmen der Fehler beschreiben lässt \cite{Atkinson2021}.
In Softwarebibliotheken wird der Parameter $\lambda$ datengetrieben bestimmt.
In Python übernimmt dies beispielsweise die Bibliothek \texttt{scipy}, indem $\lambda$ 
intern per Maximum-Likelihood geschätzt wird \cite{scipy-boxcox}.

Die Wirkung der Transformationen wird in Abbildung \ref{fig:log_transform_hist} exemplarisch gezeigt.
Links ist die rechtsschiefe Verteilung auf Originalskala dargestellt, in der Mitte dieselben Werte 
nach der Transformation $\log(1+x)$ und rechts nach einer Box-Cox-Transformation.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{images/transform_demo_log1p_vs_boxcox_kde.png}
  \caption{Beispiel einer rechtsschiefen Verteilung vor und nach Transformationen. 
  Die überlagerte Dichtekurve verdeutlicht die Form der Verteilung.}
  \label{fig:log_transform_hist}
\end{figure}

\iffalse
\subsection{Automatisierung, Reproduzierbarkeit und Datenqualität}
Eine \emph{skriptbasierte}, reproduzierbare Umsetzung (z.\,B.\ in~R oder Python) 
gewährleistet konsistente Analysen und Nachvollziehbarkeit aller Berechnungsschritte. 
Im Rahmen der explorativen Datenanalyse unterstützt sie die Datenqualitätsbewertung durch:
Erkennung von Datenfehlern, Ausreißern und fehlenden Werten, 
und die Überwachung einfacher Profilierungsmaße (z.\,B.\ Anteil fehlender oder eindeutiger Werte) 
zur Bewertung von Vollständigkeit und Eindeutigkeit.
Diese systematische, nachvollziehbare Vorgehensweise bildet die Grundlage 
für die Qualitätssicherung entlang der Pipeline \emph{PDF~$\rightarrow$~JSON} 
und schafft Transparenz über den Zustand der analysierten Daten.
\fi

\subsection{Hauptkomponentenanalyse (PCA)}
Die \emph{Hauptkomponentenanalyse} (Principal Component Analysis, PCA) ist eine weit 
verbreitete multivariate statistische Methode zur
Vereinfachung komplexer Datensätze, 
Extraktion der wesentlichen Informationen und der Erkennung von Strukturen in den Daten.
Ihre mathematischen Grundlagen beruhen auf fundamentalen Konzepten der linearen Algebra. \cite{Abdi2010}

Die PCA verfolgt zwei zentrale Ziele:

\paragraph{Reduktion der Dimensionalität}

Die ursprünglichen Variablen werden durch eine kleinere Menge neuer, 
orthogonaler Variablen ersetzt, die \emph{Hauptkomponenten}. Diese werden als 
lineare Kombinationen der Ausgangsvariablen konstruiert und erklären maximal 
mögliche Varianz. Die erste Hauptkomponente trägt dabei den größten Anteil 
der Gesamtvarianz. \cite{Maćkiewicz1993}

\paragraph{Vereinfachung und Interpretation}

Durch Komprimierung ohne wesentlichen Informationsverlust ermöglicht die PCA 
eine vereinfachte Darstellung des Datensatzes sowie die Analyse von 
Beziehungen zwischen Beobachtungen und Variablen. Muster von Ähnlichkeiten 
können grafisch auf wenigen Achsen dargestellt werden. \cite{Abdi2010}
 


\paragraph{Mathematische Grundlagen}

Die PCA beruht darauf, aus einer Menge korrelierter Variablen neue,
unkorrelierte Variablen (\emph{Hauptkomponenten}) zu erzeugen. Dafür wird die
Varianz-Kovarianz-Matrix $S$ zerlegt \cite{Maćkiewicz1993}. 
Die Hauptkomponenten ergeben sich aus den Eigenwerten
und Eigenvektoren dieser Matrix. Dazu wird
\[
  |S - \lambda I| = 0
\]
gelöst. Die Eigenwerte $\lambda_i$ geben an, wie viel Varianz jede
Hauptkomponente erklärt. Die zugehörigen Eigenvektoren liefern die
Gewichte der linearen Kombinationen, aus denen die Hauptkomponenten
entstehen:
\[
  V = A^{\prime} X.
\]
\cite{Maćkiewicz1993}
Die Hauptkomponenten sind unkorreliert \cite{Abdi2010}.

\paragraph{Singulärwertzerlegung}

Statt über Eigenwerte kann die PCA auch mit der
Singulärwertzerlegung (SVD) berechnet werden, wie es in modernen Softwarebibliotheken (z.B.
Python scikit-learn \cite{scikit-learn-pca} und Matlab \cite{matlab-pca}) üblich ist:
\[
  X = P \Lambda Q^{T}.
\]
Die quadrierten Singulärwerte entsprechen dabei den Eigenwerten, und die
Hauptkomponenten der Beobachtungen ergeben sich zu
\[
  F = P \Lambda.
\]
Die SVD zeigt, dass die PCA eine optimale Niedrigrang-Approximation der
Datenmatrix im Sinne der kleinsten Quadrate liefert. \cite{Abdi2010}

\paragraph{PCA in dieser Arbeit}

Die Hauptkomponentenanalyse bildet die Grundlage der Principal Component 
Regression (PCR), bei der die ursprünglichen Regressorvariablen durch 
die Hauptkomponenten ersetzt werden. Dies ist insbesondere dann 
vorteilhaft, wenn starke Multikollinearität zwischen den Variablen besteht,
da die Schätzung der Regressionskoeffizienten durch die Entkorrelierung 
wesentlich stabiler wird. \cite{Jolliffe1982}
Da die in den PEP-Daten enthaltenen Materialanteile 
teils ausgeprägt korrelieren, wird die lineare Regression in dieser Arbeit 
auf Basis der zuvor berechneten Hauptkomponenten durchgeführt.

\subsection{Lineare Regression}

Die lineare Regression dient in dieser Arbeit als methodische Grundlage zur 
Modellierung der Umweltwirkungen von Produkten auf Basis quantitativer Einflussgrößen. 
Ziel ist es, Zusammenhänge zwischen erklärenden Variablen wie \emph{Produktgewicht}, 
\emph{Materialzusammensetzung}, \emph{Stromverbrauch} und \emph{verwendetem Energiemix}
und den resultierenden \emph{Umweltindikatoren} zu quantifizieren und zur Abschätzung unbekannter 
Werte nutzbar zu machen.

\paragraph{Modellstruktur}
Das Regressionsmodell beschreibt den linearen Zusammenhang zwischen einer abhängigen Variable 
\( y \) (z.\,B.\ einem Umweltindikator)
und mehreren unabhängigen Variablen 
\( x_1, x_2, \dots, x_k \) (z.\,B.\ Gewicht, Stromverbrauch, Materialanteile):
\[
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_k x_k + \varepsilon.
\]
Dabei ist \( \beta_0 \) der Achsenabschnitt, \( \beta_i \) die Regressionskoeffizienten der jeweiligen Einflussgrößen 
und \( \varepsilon \) ein zufälliger Fehlerterm, der unerklärte Varianzanteile abbildet.
Die Koeffizienten \(\beta_i\) quantifizieren die Richtung und Stärke des Einflusses einzelner Variablen auf den Zielindikator. \cite{Montgomery2022}

\paragraph{Zentrale Annahmen}
Für die lineare Regression gelten folgende Grundannahmen:
\begin{itemize}
    \item \textbf{Linearität:} Die Beziehung zwischen abhängiger und unabhängigen Variablen ist näherungsweise linear.
    \item \textbf{Erwartungswert Null:} Die Fehlerterme haben einen Erwartungswert von null \(E(\varepsilon)=0\) und sind Normalverteilt.
    \item \textbf{Homoskedastizität:} Die Varianz der Fehler ist konstant und unabhängig von den erklärenden Variablen.
    \item \textbf{Unabhängigkeit:} Die Fehlerterme sind voneinander unkorreliert.
\end{itemize}
\cite{Su2012}
Diese Annahmen sichern die Unverzerrtheit und Effizienz der Parameterschätzungen. 
Für explorative Anwendungen, wie sie in dieser Arbeit verfolgt werden, steht jedoch die Strukturentdeckung im Vordergrund.
Moderate Abweichungen von den Idealannahmen sind daher akzeptabel, sofern sie dokumentiert werden.


\paragraph{Parameterschätzung}
Die Schätzung der Regressionskoeffizienten erfolgt nach der Methode der kleinsten Quadrate (\emph{Ordinary Least Squares}, OLS). 
Dabei werden die Parameter so bestimmt, dass die Summe der quadrierten Abweichungen zwischen 
beobachteten und modellierten Werten minimal wird:
\[
S(\boldsymbol{\beta}) = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2.
\]
Die resultierenden Schätzer sind unter den genannten Modellannahmen unverzerrt 
und besitzen die kleinste Varianz unter allen linearen, unverzerrten Schätzverfahren (Gauss-Markov-Eigenschaft). \cite{Su2012}

\paragraph{Modellinterpretation}
Die Koeffizienten \(\beta_i\) geben an, wie stark sich der Zielindikator \(y\) im Mittel verändert, 
wenn sich die Einflussgröße \(x_i\) um eine Einheit ändert, während alle anderen Variablen konstant bleiben. 
Das \emph{Bestimmtheitsmaß} \(R^2\) beschreibt den Anteil der Varianz des Zielindikators, der durch die 
erklärenden Variablen erklärt wird, und dient als zentrales Maß der Modellgüte. \cite{Montgomery2022}
Der \emph{Root Mean Square Error} (RMSE) misst die durchschnittliche Abweichung zwischen 
beobachteten und vorhergesagten Werten. Er hat die gleiche Einheit wie die Zielvariable und 
ist dadurch leicht interpretierbar und ebenfalls ein Indikator für die Modellgüte. \cite{Montgomery2022}


\paragraph{Anwendungsrahmen}
In dieser Arbeit wird die multiple lineare Regression verwendet, 
um Heuristiken zur Abschätzung der Umweltwirkungen von Elektro- und Elektronikprodukten zu entwickeln.
Das Modell dient der quantitativen Erfassung
von Zusammenhängen zwischen Produktmerkmalen und Umweltindikatoren und daraus schließend der möglichst präzisen Prognose 
der Umweltindikatoren anhand der Input-Variablen.
Damit bildet die lineare Regression eine nachvollziehbare, statistisch fundierte Basis für die 
Entwicklung eines vereinfachten Bewertungsmodells innerhalb der PEP-Datenanalyse.
